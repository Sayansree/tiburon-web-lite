<!doctype html>
<html lang="en">

<head>
  <!--<h2>hello</h2> -->
  <title>Team Tiburon &mdash; The AUV Team Of NITR</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="shortcut icon" href="https://github.com/auvnitrkl/webAssets/blob/main/images/favicon.ico?raw=true">
  <link rel="stylesheet" href="css/custom-bs.css">
  <link rel="stylesheet" href="css/jquery.fancybox.min.css">
  <link rel="stylesheet" href="fonts/icomoon/style.css">
  <link rel="stylesheet" href="fonts/line-icons/style.css">

  <!-- MAIN CSS -->
  <link rel="stylesheet" href="css/style.css">
  <!-- FONTAWESOME -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.2.0/css/all.min.css">
  
</head>

<body>
  <!-- it is used to show the spining circle before loading the page-->
  <div class="loader">
    <div class="spinner-border text-primary" role="status">
      <span class="sr-only">Loading...</span>
    </div>
  </div>
  <!-- use for menu in narrow screen mode-->
  <div class="site-wrap">
    <div class="site-mobile-menu site-navbar-target">
      <div class="site-mobile-menu-header">
        <img style="margin-left: 10px;  padding: 5px;" class="img-fluid" height="45" width="45" src="https://github.com/auvnitrkl/webAssets/blob/main/images/logo.png?raw=true"></a>
        <div class="site-mobile-menu-close mt-3">
          <span class="icon-close2 js-menu-toggle"></span>
        </div>
      </div>
      <div class="site-mobile-menu-body"></div>
    </div> <!-- .site-mobile-menu -->
    <!-- NAVBAR -->
    <header class="site-navbar" id="top">
      <div class="container-fluid">
        <div class="dark-div">
          <button onclick="darkTheme()">DM</button>
        </div>
        <div class="row align-items-center">
          <div class="site-logo col-8"><a href="index.html">
              TIBURON</a>
            <a href="index.html"><img style="margin-left: 10px;  padding: 5px;" class="img-fluid" height="45" width="45" src="https://github.com/auvnitrkl/webAssets/blob/main/images/logo.png?raw=true"></a>
            <iframe src="https://www.facebook.com/plugins/like.php?href=https%3A%2F%2Fwww.facebook.com%2Ftiburonauv%2F&amp;width=72&amp;layout=button_count&amp;action=like&amp;size=small&amp;show_faces=true&amp;share=true&amp;height=21&amp;appId"
              width="140" height="20" style="border:none;overflow:hidden" scrolling="no" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>
          </div>
          <nav class="mx-auto site-navigation">
            <ul class="site-menu js-clone-nav d-none d-lg-block">
              <li><a href="index.html">Home</a></li>
              <li><a href="team.html">The Team</a></li>
              <li><a href="achievements.html">Achievements</a></li>
              <li><a href="gallery.html">Gallery</a></li>
              <li><a href="events.html">Technical Events</a></li>
               <li><a href="robosub21.html">Robosub 2021</a></li>
              <li><a href="sponsors.html">Sponsors</a></li>
              <li><a href="contact.html">Contact Us</a></li>
            </ul>
          </nav>
          <div class="col-4 site-burger-menu d-block d-lg-none text-right">
            <a href="#" class="site-menu-toggle js-menu-toggle"><span class="icon-menu h3"></span></a>
          </div>
        </div>
      </div>
    </header>
    <section class="site-navbar2" id="top2">
      <div>
        <img class="img-fluid" height="75" width="75" src="https://github.com/auvnitrkl/webAssets/blob/main/images/logo.png?raw=true">
      </div>
    </section>
    <!-- HOME -->
    <section class="home-section section-hero overlay slanted" id="home-section">
      <div class="container">
        <div class="row align-items-center justify-content-center">
          <div class="col-md-8 text-center">
            <h1><strong>Team Tiburon</strong></h1>
            <h2 style="color:cornsilk;">The AUV Team Of NIT Rourkela</h2>
            <div class="mx-auto w-75">
              <!--need to change this line -->
              <h3 style=" color:rgb(191, 191, 191)">Group of committed engineers working to design the best tech to understand the marine ways.</h3>
            </div>
            <p class="mt-5"><a href="#inductions" class="btn btn-outline-white btn-md ">INDUCTION PROCESS</a></p>
          </div>
        </div>
      </div>
      <!-- VIDEO -->
      <div class="video-container">
        <video autoplay loop="true" controls muted>
          <source type="video/mp4" src="https://github.com/auvnitrkl/webAssets/blob/main/videos/tiburon.mp4?raw=true">
        </video>
      </div>
      <a href="#about-us-section" class="scroll-button smoothscroll">
        <span class=" icon-keyboard_arrow_down"></span>
      </a>
    </section>
    <!-- ABOUT US -->

    <section class="site-section about-us-section container">
        <div class="row m-3 bg-success">
            <div class="col-lg-6 col-sm-12 d-flex justify-content-center">
                <h1>ROBOSUB 2021</h1>
            </div>
            <div class="col-lg-6 col-sm-12 d-flex justify-content-center">
                <button class="btn-dark m-1 w-25"><a class="text-light" href="./tdr/SoftwareTDR.pdf">Software TDR</a></button>
                <button class="btn-dark m-1 w-25"><a class="text-light" href="./tdr/TiburonTDR.pdf">Electronics TDR</a></button>
                <button class="btn-dark m-1 w-25"><a class="text-light" href="./tdr/MechTDR.pdf">Mechanical TDR</a></button>
            </div>
        </div>
<!--  software----------------------------------------------------------------/////////////////////////////////////////////////////////////////////////////////////////////////////////////-->
        <h1>Software Subsystem</h1>
        <div>
            <h3>Visual Object Detection:</h3> 
            <b>Computer Vision(CV)</b> <br>
            <ol>
                <li><b>OpenCV:</b> We as a team have been using OpenCV to perform image processing techniques to extract information from camera frames and detect obstacles & tasks at hand in many of our previous competitions. We used bounding box vertices to locate tasks such as gates & bins. These detections also helped in localizing our bot as by applying basic math similarity concept we were able to estimate the position of our bot with respect to task (having the focal length of the camera at hand and the original length and width of task objects and it comparing with the ratio of pixel lengths of edges). The center we calculated from the contours/hough lines made around the task(gates) helped in aligning the bot and pass through the gate.
                    Neural Network</li>
                <li><b>YOLOv3/v5:</b> Although traditional image processing methods were successful in detecting objects underwater(with suitable adjustment in HSV values as per lighting) but it comes with certain drawbacks such as failing to detect from a greater distance and unreliable results in different lighting and background. Here we have found Yolov3(You Only Look Once) CNN ( a real-time object detection algorithm that identifies specific objects in videos, live feeds, or images.)to be effective. We realized if we can navigate to the object close enough using this neural network-based model, then we can apply suitable thresholding techniques or can use the same model to get the center and pass through the target(gate) or reach the task at hand. For training the CNN, images were collected from previous competitions. Data augmentation was also performed to introduce variation and increase the dataset. We used our own Labeler Toolbox to label the images. 1/5th of the images were reserved as the test set to run the inference and the model(based on open-source neural network framework Darknet) was trained on the rest of the data. Unfortunately, we could not test the model real-time underwater on Nvidia Jetson due to the pandemic, but we still achieved very encouraging results testing it on our local systems(25 fps 87%).
                    Together with new machine learning approaches, traditional image processing techniques, color correction algorithms, and other fine-tuned approaches, we hope to strengthen the perception system of Makara 4.0
                </li>
            </ol>
            <h3>Better Perception Techniques:</h3>
            <ol>
                <li><b>Glare Removal</b>Sometimes the images obtained from the bottom camera are too bright and keep fluctuating due to sunlight reflection(happens when the arena isn't that deep and has a shiny bottom surface). To remove glares from these images we have used an algorithm to make them more visible and clear.
                    The algorithm keeps track of the most recent 'n' images and finds the median, mean variation of 'n' RGB values respectively for each pixel in the image. The final image obtained has less glare and the lighting is spread equally. As the glares are rapidly moving due to surface water motion, taking its median helps to distribute its intensity and makes the image more clear and visible.
                </li>

                <li><b>Color Correction</b>The Robosub arena watercolor is greenish and unclear and thus making things difficult for the object detection model as they are trained on a different environment.
                    To change the color and make it more natural blue, we have simply changed the weight of the RGB channel of the image (called Stretching) and obviously, the weight of the green channel needs to be reduced. Then to remove the fogginess in the image we used the Contrast Limited Adaptive Histogram Equalization (CLAHE) algorithm that helped to increase the image contrast and made images more clear.
                </li>
            </ol>

            

            <h3>Model Optimisation:</h3>
            <p>The perception stack is mostly carried out by object detection methods using deep neural networks. However, there has always been a trade-off between accuracy and real-time fps. We have approached this problem with model optimization using quantization by implementing state-of-the-art quantization-aware training methods. This approach simply converts all float32 weights and activations to int8 thus drastically reducing model size as well as the inference time. This enables us to use any low-power computing devices and stack multiple models in our pipeline without compromising performance.
            </p>

            <h3>Stereo Vision:</h3>
            <p>In traditional stereo vision, two cameras, displaced horizontally from one another are used to obtain two differing views on a scene, in a manner similar to human binocular vision. By comparing these two images, the relative depth information can be obtained in the form of a disparity map, which encodes the difference in horizontal coordinates of corresponding image points. Computer vision algorithms are also applied to images for smoothening and rectification of the images. Thus by using stereo vision we can perceive the distance between the object and the camera plane while carrying out various tasks during the mission.</p>

            <h3>PID Auto-tuning:</h3>
            <p>The PID controllers used in our AUV have to tune it (that means to obtain the value of Kp, Ki, Kd constants ) manually so that it manoeuvres properly.
                But this time we tried to automate things and decided to build a tuning algorithm using Ant Colony Optimization (ACO). To tune the constants of each degree of freedom, the algorithm first generates some random values and tries to minimize the error generated from a cost function for each generated value and then determine the best values obtained from this generation and use them as the parents for the next generation. This happens generation over generation till we get an optimal value and every generation is more optimal than its previous one.</p>

            <h3>Working with DVL and why ?</h3>
            <p>
                A Doppler Velocity Log (DVL) is an acoustic sensor that estimates velocity relative to the sea bottom. This is achieved by sending a long pulse along with a minimum of three acoustic beams, each pointing in a different direction. Typically, this produces estimates of velocity converted into an XYZ coordinate frame of reference – the DVL’s frame of reference. Together with a heading estimate, these velocity estimates may be integrated over the ping interval to estimate a step-by-step change of position – i.e. displacement = velocity × time step.
                <br>
                It is important to ensure that velocity estimates do not have any bias or offsets because this will lead to a growing error in the position estimate. This is where the Doppler Velocity Log becomes a key in the subsea navigation for its accurate estimate of velocity with zero-mean bias. This sensor combined with the IMU ensures a proper estimation of obstacles/key places that the vehicle needs to avoid/reach to perform some specific work.
                <br>
            </p>
        </div>
<!--        electronics--------------------------------------------------------////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////  -->

         <h1>Electronics Subsystem</h1>
        <div>
            <h3>OVERVIEW:</h3> 
            <p>Though the previous version of our electronics stack met all the requirements perfectly, but, it also has its issues that we have addressed in this year’s rebuild. </p>
            <p>One major issue was power management, previously we used to power the thrusters using one battery and rest of the circuit using another. This caused the thruster power to run out quickly than the other which reduced our bots runtime and efficiency, so this year we switched to a single power source. We now use a single battery at a time and after it runs out we switch to another using hot swap controllers.This results in increased runtime and better power management.
</p>
<p>This year, we have decided to change the Microcontroller platform from ATMEL to STM32 line of microcontrollers giving us the flexibility to use High processing power where required or Low power usage if needed. 
</p>
<p>Last year’s stack missed critical security features like Under and overvoltage protection, current spike and overcurrent protection. So we have added this protection using eFuses and TVS diodes. The thought of security leads us to the Kill switch, last year we used a REED switch as a kill switch which was extremely unreliable, and had a lot of false firing issues, this year we switched to a better and more reliable waterproof physical button as a kill switch. 
</p>Another major issue was the lack of any “Debugging data” making any sort of debugging a major pain in the buttocks, so we have introduced an independently powered (2x18650 cells) custom made debugging board, which allow us to keep track of Voltages of various nodes of the circuit along with current going to each thruster and temperatures of ESCs. This data will be sent to the main CPU using CAN protocol (which will also be the standard protocol all over the stack) then the CPU can use this data in intelligent decision making like shutting down parts of the stack in case of local errors and saving the rest of the circuit and AUV. the board will also have an EEPROM to store all this data for later use in analytics. </p>
<p>Following the debugging line of thought, in our previous stack making any kind of repair was also difficult, so this time we have decided to replace a single PCB with various modularized PCBs making it easy to just replace the problematic board with a replacement much like any other commercial electronics gadget. Also, this allows our debugging board to execute the local shutdown of unimportant components in case of an error in the board. 
</p>
            <h3>POWER MANAGEMENT BOARD:</h3>
<p>As already discussed due to galvanic isolation, we powered the Thrusters with one battery and rest with the another. The Thruster battery discharged fast, decreasing runtime and efficiency. So, this year a smart battery management system was incorporated which used 2 16000 mAh LIPO battery packs but one battery at a time.After a battery discharges to a minimum preset level we switch batteries with help of a hot swap controller and a STM32f103c8t6. This board is also responsible for generation of different voltage levels,i.e. 12V,5V and 3.3V to meet the requirements of different components across our circuit.</p>
            <h3>BACKPLANE/MAIN BOARD:</h3>

<p>All the different boards are connected to the main board using high density connectors which are controlled by STMH745ZI-Q. This board communicates with JETSON TX2.</p>
<p>A high-quality, reliable Inertial Measurement Unit (IMU) is a crucial part of any autonomous underwater vehicle (AUV). Previously our AUV had to rely on accelerometer and gyroscope signals to obtain velocity and position values, which was giving lots of error in determining the exact position. So to assist our previous IMU, we are installing Doppler Velocity Log (DVL) in Makara which serially communicates to the CPU. Erroneous velocity values that are impossible or unlikely are filtered out. For samples when no values are received, the program extrapolates until a real value is received.
</p>
<p>Furthermore, to provide an accurate way of measuring depth, a pressure-based depth sensor was also added to our AUV which communicates with STM32 microcontroller through I2C.
</p>
<h3>ACTUATOR BOARD:
</h3>
<p>The AUV has a support for 10 thrusters and also we have incorporated MOSFETs to drive solenoid valves for torpedoes. We use a mix of T200 and T100 thrusters, controlled using BlueESC. We are also planning to introduce current sensors to measure the current going to individual thrusters thus helping us to get an approximate idea for speed of thrusters as we can’t have an encoder on the thrusters. 
</p>
<h3>DEBUGGING BOARD:</h3>
<p>In our previous circuit, we used LEDs to debug circuits which made troubleshooting difficult and time consuming. So, in the new circuit we introduced an independent debug/protection board with an onboard OLED screen powered with an independent power supply. This board contains a STM32f103c8t6 microcontroller which monitors voltage, current and temperature at all important nodes using LTC2990. The data acquired is stored in an EEPROM which can be used for later debugging. Our circuits have also been incorporated with TPS2660x efuses to ensure circuit protection. These efuses can also be controlled by the debugging board in case of any emergency.
</p>
<h3>ACOUSTIC BOARD:</h3>
<p>n our previous bot, we tried making passive sonar for locating the pinger with hydrophone and RaspberryPi with external ADC but that did not turn out to be working. So This time we planned the whole Acoustic signal processing part based on FPGA. We designed our own Analog frontend board which Has Adjustable Bandpass filters, Amplifiers with Adaptive gain control, and High-speed ADCs which connect to the Zybo Z7 FPGA board. We find out the pinger heading by finding the Time Delay of Arrival of signals. FPGA can communicate to onboard Computer via Ethernet or CAN bus
</p>
<p>There is an onboard AVR microcontroller ATMEGA328p that controls the Frontend gain and filtering also controls various power rails within the board. This board has its own local power distribution to meet the special requirements of the board. The microcontroller also interfaces with the debugging board via CAN and can regulate high-performance and power-saving modes. And power down partially or fully in case of fault when signaled from debugging board.
</p>

            
        </div>
<!--  mechanical---------------------------------------////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////-->
         <h2>Mechanical Subsystem</h2>
        <div>
            <h3>Visual Object Detection:</h3> 
            <b>Computer Vision(CV)</b> <br>
            <ol>
                <li><b>OpenCV:</b> We as a team have been using OpenCV to perform image processing techniques to extract information from camera frames and detect obstacles & tasks at hand in many of our previous competitions. We used bounding box vertices to locate tasks such as gates & bins. These detections also helped in localizing our bot as by applying basic math similarity concept we were able to estimate the position of our bot with respect to task (having the focal length of the camera at hand and the original length and width of task objects and it comparing with the ratio of pixel lengths of edges). The center we calculated from the contours/hough lines made around the task(gates) helped in aligning the bot and pass through the gate.
                    Neural Network</li>
                <li><b>YOLOv3/v5:</b> Although traditional image processing methods were successful in detecting objects underwater(with suitable adjustment in HSV values as per lighting) but it comes with certain drawbacks such as failing to detect from a greater distance and unreliable results in different lighting and background. Here we have found Yolov3(You Only Look Once) CNN ( a real-time object detection algorithm that identifies specific objects in videos, live feeds, or images.)to be effective. We realized if we can navigate to the object close enough using this neural network-based model, then we can apply suitable thresholding techniques or can use the same model to get the center and pass through the target(gate) or reach the task at hand. For training the CNN, images were collected from previous competitions. Data augmentation was also performed to introduce variation and increase the dataset. We used our own Labeler Toolbox to label the images. 1/5th of the images were reserved as the test set to run the inference and the model(based on open-source neural network framework Darknet) was trained on the rest of the data. Unfortunately, we could not test the model real-time underwater on Nvidia Jetson due to the pandemic, but we still achieved very encouraging results testing it on our local systems(25 fps 87%).
                    Together with new machine learning approaches, traditional image processing techniques, color correction algorithms, and other fine-tuned approaches, we hope to strengthen the perception system of Makara 4.0
                </li>
            </ol>
            
        </div>




    </section>
 

  <footer class="site-footer slanted-footer">

    <a href="index.html#home-section" class="smoothscroll scroll-top">
      <span class="icon-keyboard_arrow_up"></span>
    </a>
    <div class="container">
      <div class="row mb-5">
        <div class="col-6 col-md-3 mb-4 mb-md-0">
          <h3>Technical Events</h3>
          <ul class="list-unstyled">
            <li><a href="https://sauvc.org/">Singapore AUV Challenge(SAUVC)</a></li>
            <li><a href="https://www.niot.res.in/SAVe/">Student Autonomous Underwater Vehicle(SAVe) Competition</a></li>
            <li><a href="https://robosub.org/">RoboSub 2019</a></li>
          </ul>
        </div>
        <div class="col-6 col-md-3 mb-4 mb-md-0">
          <h3>Address</h3>
          <ul class="list-unstyled">
            <li>Technology Innovation and Industry Relations Building, NIT Rourkela</li>
            <li><a href="mailto:tiburonnitr.contact@gmail.com">tiburonnitr.contact@gmail.com</a></li>
          </ul>
        </div>
        <div class="col-6 col-md-3 mb-4 mb-md-0">
          <h3>Faculty Advisor</h3>
          <ul class="list-unstyled">
            <li>Dr Haraprasad Roy</li>
            <li><a href="mailto:royh@nitrkl.ac.in">royh@nitrkl.ac.in</a></li>
          </ul>
        </div>
        <div class="col-6 col-md-3 mb-4 mb-md-0">
          <h3>Contact Us</h3>
          <div class="footer-social">
            <a href="https://www.facebook.com/tiburonauv/"><span class="icon-facebook"></span></a>
            <a href="https://www.twitter.com/auvnitrkl"><span class="icon-twitter"></span></a>
            <a href="https://www.instagram.com/auvnitrkl"><span class="icon-instagram"></span></a>
            <a href="https://www.linkedin.com/school/tiburonauv"><span class="icon-linkedin"></span></a>
            <!--<a href="#"><span class="icon-linkedin"></span></a>-->
          </div>
        </div>
      </div>

      <div class="row text-center">
        <div class="col-12">
          <p class="copyright"><small class="block">
              <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
              Copyright &copy;<script>
                document.write(new Date().getFullYear());
              </script> All rights reserved | Developed from
              <a href="https://colorlib.com" target="_blank">Colorlib</a></a>
              <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            </small></p>
        </div>
      </div>
    </div>
  </footer>

  </div>

  <!-- SCRIPTS -->
  <script src="js/jquery.min.js"></script>
  <script src="js/bootstrap.bundle.min.js"></script>
  <script src="js/isotope.pkgd.min.js"></script>
  <script src="js/stickyfill.min.js"></script>
  <script src="js/jquery.fancybox.min.js"></script>
  <script src="js/jquery.easing.1.3.js"></script>
  <script src="js/readmore.js"></script>
  <script src="js/jquery.waypoints.min.js"></script>
  <script src="js/jquery.animateNumber.min.js"></script>
  <script src="js/slide-show.js"></script>
  <script src="js/custom.js"></script>
  <script src="js/dark.js"></script>
</body>

</html>
